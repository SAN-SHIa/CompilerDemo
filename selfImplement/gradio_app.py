#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LR(1)ç¼–è¯‘åŸç†åˆ†æå™¨ Gradio å¯è§†åŒ–ç•Œé¢
"""

import gradio as gr
import subprocess
import os
import tempfile
import shutil
from pathlib import Path
import re

# å¯¼å…¥Python LR(1)åˆ†æå™¨
try:
    from python_lr1_parser import create_c_grammar, LR1Parser
    from lr1_visualizer import (DFAVisualizer, SyntaxTreeBuilder, SyntaxTreeVisualizer,
                               generate_lr1_table_markdown, generate_analysis_steps_markdown)
    PYTHON_BACKEND_AVAILABLE = True
except ImportError as e:
    print(f"Pythonåç«¯ä¸å¯ç”¨: {e}")
    PYTHON_BACKEND_AVAILABLE = False

# å·¥ä½œç›®å½•è®¾ç½®
WORK_DIR = Path(__file__).parent.resolve()
CPP_EXECUTABLE = WORK_DIR / "exe" / "main"
OUTCOME_DIR = WORK_DIR / "outcome"

# é¢„å®šä¹‰çš„æ–‡æ³•é€‰é¡¹ - ç®€åŒ–ç‰ˆæœ¬ï¼Œåªä¿ç•™èƒ½æ­£ç¡®å¤„ç†çš„
GRAMMARS = {
        "Cè¯­è¨€1: ç®€å•èµ‹å€¼è¯­å¥ - Sâ†’id=E, Eâ†’E+T|T, Tâ†’T*F|F, Fâ†’(E)|id|num": {
        "number": 7,
        "productions": ["S â†’ id=E;", "E â†’ E+T", "E â†’ T", "T â†’ T*F", "T â†’ F", "F â†’ (E)", "F â†’ id", "F â†’ num"],
        "example_input": "result = value + factor * 5;"
    },
    "Cè¯­è¨€2: ifè¯­å¥ - Sâ†’if(E)S|id=E, Eâ†’E==T|T, Tâ†’id|num": {
        "number": 8,
        "productions": ["S â†’ if(E)S", "S â†’ id=E;", "S â†’ {SL}", "SL â†’ S", "SL â†’ S SL", "E â†’ E==T", "E â†’ T", "T â†’ id", "T â†’ num"],
        "example_input": "if (count == 10) {\n    result = success;\n}"
    },
    "Cè¯­è¨€3: å˜é‡å£°æ˜ - Sâ†’T id, Tâ†’int|float|char": {
        "number": 9,
        "productions": ["S â†’ T id;", "S â†’ T id=E;", "T â†’ int", "T â†’ float", "T â†’ char", "E â†’ id", "E â†’ num"],
        "example_input": "int counter = 0;"
    },
    "Cè¯­è¨€4: whileå¾ªç¯ - Sâ†’while(E)S|id=E, Eâ†’E<T|T, Tâ†’T+F|F, Fâ†’id|num": {
        "number": 10,
        "productions": ["S â†’ while(E)S", "S â†’ id=E;", "S â†’ {SL}", "SL â†’ S", "SL â†’ S SL", "E â†’ E<T", "E â†’ T", "T â†’ T+F", "T â†’ F", "F â†’ id", "F â†’ num"],
        "example_input": "while (i < 10) {\n    i = i + 1;\n}"
    },
    "Cè¯­è¨€5: ç®—æœ¯è¡¨è¾¾å¼ - Eâ†’E+T|E-T|T, Tâ†’T*F|T/F|F, Fâ†’(E)|id|num": {
        "number": 11,
        "productions": ["E â†’ E+T", "E â†’ E-T", "E â†’ T", "T â†’ T*F", "T â†’ T/F", "T â†’ F", "F â†’ (E)", "F â†’ id", "F â†’ num"],
        "example_input": "(a + b) * c / (d - 2);"
    },
    "æ–‡æ³•1: Sâ†’CC, Câ†’cC|d": {
        "number": 1,
        "productions": ["S â†’ CC", "C â†’ cC", "C â†’ d"],
        "example_input": "ccdccd"
    },
    "æ–‡æ³•2: Sâ†’L=S|R, Lâ†’aLR|b, Râ†’a": {
        "number": 2, 
        "productions": ["S â†’ L=S", "S â†’ R", "L â†’ aLR", "L â†’ b", "R â†’ a"],
        "example_input": "aba=b=a"
    },
    "æ–‡æ³•3: Sâ†’aLb|a, Lâ†’aR, Râ†’LR|b": {
        "number": 3,
        "productions": ["S â†’ aLb", "S â†’ a", "L â†’ aR", "R â†’ LR", "R â†’ b"],
        "example_input": "aaabbb"
    },
    "æ–‡æ³•4: Sâ†’L=LR|R, Lâ†’aR|b, Râ†’L": {
        "number": 4,
        "productions": ["S â†’ L=LR", "S â†’ R", "L â†’ aR", "L â†’ b", "R â†’ L"],
        "example_input": "b=abab"
    },
    "æ–‡æ³•5: Sâ†’(L)|a, Lâ†’L,S|S": {
        "number": 5,
        "productions": ["S â†’ (L)", "S â†’ a", "L â†’ L,S", "L â†’ S"],
        "example_input": "((a),a)"
    },
    "æ–‡æ³•6: Sâ†’(S)S|Îµ": {
        "number": 6,
        "productions": ["S â†’ (S)S", "S â†’ Îµ"],
        "example_input": "()()"
    }

}

def create_cpp_file_with_grammar(grammar_key, input_string):
    """æ ¹æ®é€‰æ‹©çš„æ–‡æ³•åˆ›å»ºä¸´æ—¶çš„C++æ–‡ä»¶"""
    if grammar_key not in GRAMMARS:
        return None
        
    grammar_info = GRAMMARS[grammar_key]
    grammar_number = grammar_info["number"]
    
    # è¯»å–åŸå§‹main.cppæ–‡ä»¶
    main_cpp_path = WORK_DIR / "main.cpp"
    with open(main_cpp_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # è¯»å–functions.cppæ–‡ä»¶
    functions_cpp_path = WORK_DIR / "functions.cpp"
    with open(functions_cpp_path, 'r', encoding='utf-8') as f:
        functions_content = f.read()
    
    # ä¿®æ”¹æ–‡æ³•ç¼–å·å’Œè¾“å…¥ä¸²
    # æ›¿æ¢createGrammarè°ƒç”¨ä¸­çš„å‚æ•°
    content = content.replace('createGrammar(1)', f'createGrammar({grammar_number})')
    
    # æ›¿æ¢è¾“å…¥ä¸²
    old_input_line = 'std::string inputString = "cc";'
    new_input_line = f'std::string inputString = "{input_string}";'
    content = content.replace(old_input_line, new_input_line)
    
    # å¦‚æœæ˜¯Cè¯­è¨€æ–‡æ³•ï¼ˆç¼–å·7-24ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦åœ¨functions.cppä¸­æ·»åŠ å¯¹åº”çš„æ–‡æ³•å®šä¹‰
    if grammar_number >= 7:
        functions_content = add_c_grammar_support(functions_content, grammar_number, grammar_info)
    
    # å°†#include "functions.cpp"æ›¿æ¢ä¸ºç›´æ¥åµŒå…¥functions.cppçš„å†…å®¹
    include_line = '#include "functions.cpp"'
    if include_line in content:
        content = content.replace(include_line, functions_content)
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    temp_cpp = tempfile.NamedTemporaryFile(mode='w', suffix='.cpp', delete=False, encoding='utf-8')
    temp_cpp.write(content)
    temp_cpp.close()
    
    return temp_cpp.name

def update_cpp_grammar_definitions(grammar_selections):
    """æ›´æ–°C++æ–‡ä»¶ä¸­çš„æ–‡æ³•å®šä¹‰ä»¥æ”¯æŒå¤šä¸ªæ–‡æ³•"""
    functions_cpp_path = WORK_DIR / "functions.cpp"
    main_cpp_path = WORK_DIR / "main.cpp"
    
    # è¿™é‡Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹main.cppä¸­çš„createGrammarå‡½æ•°æ¥æ”¯æŒæ›´å¤šæ–‡æ³•
    # ä¸ºç®€åŒ–ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ç°æœ‰çš„æ–‡æ³•1
    pass

def compile_and_run_analysis(grammar_key, input_string):
    """ç¼–è¯‘å¹¶è¿è¡ŒLR(1)åˆ†æ"""
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        OUTCOME_DIR.mkdir(exist_ok=True)
        
        # åˆ›å»ºå¸¦æœ‰æŒ‡å®šæ–‡æ³•çš„ä¸´æ—¶C++æ–‡ä»¶
        temp_cpp_file = create_cpp_file_with_grammar(grammar_key, input_string)
        if not temp_cpp_file:
            return "é”™è¯¯ï¼šæ— æ•ˆçš„æ–‡æ³•é€‰æ‹©", "", "", "", None
        
        # ç¼–è¯‘ä¸´æ—¶C++æ–‡ä»¶
        temp_executable = temp_cpp_file.replace('.cpp', '')
        compile_cmd = [
            "g++", "-std=c++17", "-o", temp_executable, temp_cpp_file
        ]
        
        compile_result = subprocess.run(
            compile_cmd, 
            capture_output=True, 
            text=True, 
            cwd=WORK_DIR
        )
        
        if compile_result.returncode != 0:
            os.unlink(temp_cpp_file)
            return f"ç¼–è¯‘é”™è¯¯:\n{compile_result.stderr}", "", "", "", None
        
        # è¿è¡Œåˆ†æç¨‹åº
        run_result = subprocess.run(
            [temp_executable],
            capture_output=True,
            text=True,
            cwd=WORK_DIR
        )
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_cpp_file)
        if os.path.exists(temp_executable):
            os.unlink(temp_executable)
        
        if run_result.returncode != 0:
            return f"è¿è¡Œé”™è¯¯:\n{run_result.stderr}", "", "", "", None
        
        # è¯»å–è¾“å‡ºç»“æœ
        console_output = run_result.stdout
        
        # è¯»å–ç”Ÿæˆçš„æ–‡ä»¶ï¼Œä½¿ç”¨åŠ¨æ€æ–‡ä»¶å
        grammar_info = GRAMMARS[grammar_key]
        grammar_number = grammar_info["number"]
        lr1_table_file = OUTCOME_DIR / f"lr1_table_grammar{grammar_number}.md"
        lr1_analysis_file = OUTCOME_DIR / f"lr1_analysis_grammar{grammar_number}.md"
        dfa_image_file = OUTCOME_DIR / f"dfa_grammar{grammar_number}.png"
        
        # å¦‚æœåŠ¨æ€æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤çš„grammar1æ–‡ä»¶
        if not lr1_table_file.exists():
            lr1_table_file = OUTCOME_DIR / "lr1_table_grammar1.md"
        if not lr1_analysis_file.exists():
            lr1_analysis_file = OUTCOME_DIR / "lr1_analysis_grammar1.md"
        if not dfa_image_file.exists():
            dfa_image_file = OUTCOME_DIR / "dfa_grammar1.png"
        
        table_content = ""
        analysis_content = ""
        dfa_image_path = None
        syntax_tree_path = None  # æ·»åŠ è¯­æ³•æ ‘è·¯å¾„å˜é‡
        
        if lr1_table_file.exists():
            with open(lr1_table_file, 'r', encoding='utf-8') as f:
                table_content = f.read()
        
        if lr1_analysis_file.exists():
            with open(lr1_analysis_file, 'r', encoding='utf-8') as f:
                analysis_content = f.read()
        
        if dfa_image_file.exists():
            dfa_image_path = str(dfa_image_file)
        
        # å°è¯•æŸ¥æ‰¾è¯­æ³•æ ‘å›¾åƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        syntax_tree_file = OUTCOME_DIR / f"syntax_tree_grammar{grammar_number}.png"
        if syntax_tree_file.exists():
            syntax_tree_path = str(syntax_tree_file)
        
        return console_output, table_content, analysis_content, dfa_image_path, syntax_tree_path
        
    except Exception as e:
        return f"æ‰§è¡Œé”™è¯¯: {str(e)}", "", "", "", None

def preprocess_c_code(code):
    """
    é¢„å¤„ç†Cè¯­è¨€ä»£ç ï¼Œè¿›è¡Œè¯æ³•åˆ†æå¹¶è½¬æ¢ä¸ºé€‚åˆè¯­æ³•åˆ†æçš„tokenåºåˆ—
    """
    import re
    
    # è®°å½•è¯¦ç»†çš„è¯æ³•åˆ†æè¿‡ç¨‹
    lexical_analysis_result = []
    
    # å»é™¤æ³¨é‡Š
    code_without_comments = re.sub(r'//.*', '', code)  # å•è¡Œæ³¨é‡Š
    code_without_comments = re.sub(r'/\*.*?\*/', '', code_without_comments, flags=re.DOTALL)  # å¤šè¡Œæ³¨é‡Š
    
    if code_without_comments != code:
        lexical_analysis_result.append(("æ³¨é‡Šå¤„ç†", f"åŸå§‹ä»£ç : {len(code)}å­—ç¬¦ â†’ å»é™¤æ³¨é‡Š: {len(code_without_comments)}å­—ç¬¦"))
    
    # å¤„ç†å¤šè¡Œä»£ç ï¼Œå°†æ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
    code_single_line = re.sub(r'\n', ' ', code_without_comments)
    # å¤„ç†å¤šä¸ªç©ºæ ¼
    code_normalized = re.sub(r'\s+', ' ', code_single_line).strip()
    
    if code_normalized != code_without_comments:
        lexical_analysis_result.append(("ç©ºç™½å¤„ç†", f"è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦: {len(code_without_comments)}å­—ç¬¦ â†’ {len(code_normalized)}å­—ç¬¦"))
    
    # å®šä¹‰Cè¯­è¨€å…³é”®å­—å’Œæ“ä½œç¬¦çš„æ˜ å°„
    token_map = {
        # å…³é”®å­—
        'int': 'int', 'float': 'float', 'char': 'char', 'void': 'void',
        'if': 'if', 'else': 'else', 'while': 'while', 'for': 'for',
        'switch': 'switch', 'case': 'case', 'default': 'default',
        
        # æ“ä½œç¬¦
        '++': '++', '--': '--', 
        '==': '==', '!=': '!=', '<=': '<=', '>=': '>=',
        '&&': '&&', '||': '||',
        '->': '->', '<<': '<<', '>>': '>>',
        
        # å•å­—ç¬¦æ“ä½œç¬¦
        '+': '+', '-': '-', '*': '*', '/': '/', '%': '%',
        '=': '=', '<': '<', '>': '>', '!': '!', '&': '&', '|': '|',
        '?': '?', ':': ':', ';': ';', ',': ',', '.': '.',
        '(': '(', ')': ')', '[': '[', ']': ']', '{': '{', '}': '}'
    }
    
    # è¯æ³•åˆ†ææ­£åˆ™è¡¨è¾¾å¼ - ç¡®ä¿å…³é”®å­—åœ¨æ ‡è¯†ç¬¦ä¹‹å‰åŒ¹é…
    patterns = [
        # åŒå­—ç¬¦æ“ä½œç¬¦å¿…é¡»åœ¨å•å­—ç¬¦æ“ä½œç¬¦ä¹‹å‰æ£€æŸ¥
        (r'\+\+|--|==|!=|<=|>=|&&|\|\||->|<<|>>', 'OP2'),
        # å…³é”®å­—å¿…é¡»åœ¨æ ‡è¯†ç¬¦ä¹‹å‰æ£€æŸ¥ï¼Œå¹¶ä¸”ä½¿ç”¨\bç¡®ä¿å®Œæ•´åŒ¹é…è¯è¾¹ç•Œ
        (r'\bint\b', 'int'),
        (r'\bfloat\b', 'float'),
        (r'\bchar\b', 'char'),
        (r'\bvoid\b', 'void'),
        (r'\bif\b', 'if'),
        (r'\belse\b', 'else'),
        (r'\bwhile\b', 'while'),
        (r'\bfor\b', 'for'),
        (r'\bswitch\b', 'switch'),
        (r'\bcase\b', 'case'),
        (r'\bdefault\b', 'default'),
        # æ ‡è¯†ç¬¦åœ¨å…³é”®å­—ä¹‹ååŒ¹é…
        (r'\b[a-zA-Z_][a-zA-Z0-9_]*\b', 'ID'),
        (r'\b\d+(?:\.\d+)?\b', 'NUM'),
        (r'[+\-*/=%<>!&|?:;,.\[\](){}]', 'OP1'),
        (r'\s+', 'SPACE')
    ]
    
    tokens = []
    detailed_tokens = []  # å¸¦æœ‰è¯¦ç»†ä¿¡æ¯çš„tokenåˆ—è¡¨
    pos = 0
    debug = True  # å¯ç”¨è°ƒè¯•è¾“å‡º
    
    if debug:
        print(f"å¼€å§‹é¢„å¤„ç†ä»£ç : '{code_normalized}'")
        
    while pos < len(code_normalized):
        matched = False
        for pattern, token_type in patterns:
            regex = re.compile(pattern)
            match = regex.match(code_normalized, pos)
            if match:
                value = match.group(0)
                if token_type != 'SPACE':  # å¿½ç•¥ç©ºç™½å­—ç¬¦
                    if token_type == 'ID':
                        tokens.append('id')
                        detailed_tokens.append(('id', value, pos))
                        if debug:
                            print(f"æ ‡è¯†ç¬¦: '{value}' -> 'id'")
                        lexical_analysis_result.append(("æ ‡è¯†ç¬¦", f"'{value}' â†’ 'id'"))
                    elif token_type == 'NUM':
                        tokens.append('num')
                        detailed_tokens.append(('num', value, pos))
                        if debug:
                            print(f"æ•°å­—: '{value}' -> 'num'")
                        lexical_analysis_result.append(("æ•°å­—", f"'{value}' â†’ 'num'"))
                    elif token_type in ['int', 'float', 'char', 'void', 'if', 'else', 'while', 'for', 'switch', 'case', 'default']:
                        tokens.append(token_type)
                        detailed_tokens.append((token_type, value, pos))
                        if debug:
                            print(f"å…³é”®å­—: '{value}' -> '{token_type}'")
                        lexical_analysis_result.append(("å…³é”®å­—", f"'{value}' â†’ '{token_type}'"))
                    else:  # OP1 æˆ– OP2
                        tokens.append(value)
                        detailed_tokens.append((value, value, pos))
                        if debug:
                            print(f"æ“ä½œç¬¦: '{value}' -> '{value}'")
                        lexical_analysis_result.append(("æ“ä½œç¬¦", f"'{value}'"))
                else:
                    lexical_analysis_result.append(("å¿½ç•¥", f"ç©ºç™½å­—ç¬¦: '{value}'"))
                    if debug:
                        print(f"å¿½ç•¥ç©ºç™½: '{value}'")
                pos = match.end()
                matched = True
                break
        
        if not matched:
            if debug:
                print(f"è·³è¿‡æ— æ³•è¯†åˆ«çš„å­—ç¬¦: '{code_normalized[pos]}'")
            lexical_analysis_result.append(("é”™è¯¯", f"æ— æ³•è¯†åˆ«çš„å­—ç¬¦: '{code_normalized[pos]}'"))
            pos += 1  # è·³è¿‡æ— æ³•è¯†åˆ«çš„å­—ç¬¦
    
    if debug:
        print(f"é¢„å¤„ç†åçš„è¯æ³•å•å…ƒåˆ—è¡¨: {tokens}")
    
    # è¿”å›tokenåˆ—è¡¨å’Œè¯¦ç»†çš„è¯æ³•åˆ†æç»“æœ    
    return tokens, detailed_tokens, lexical_analysis_result

def generate_lexical_analysis_markdown(code, detailed_tokens, lexical_analysis_result):
    """ç”Ÿæˆè¯æ³•åˆ†æè¿‡ç¨‹çš„Markdownæ ¼å¼è¾“å‡º"""
    # æ·»åŠ åŸå§‹ä»£ç 
    md_lines = [
        "# Cè¯­è¨€è¯æ³•åˆ†æ",
        "",
        "## åŸå§‹ä»£ç ",
        "```c",
        code,
        "```",
        "",
        "## è¯æ³•åˆ†æè¿‡ç¨‹",
        ""
    ]
    
    # æ·»åŠ è¯¦ç»†çš„è¯æ³•åˆ†æè¿‡ç¨‹
    for step_type, description in lexical_analysis_result:
        md_lines.append(f"- **{step_type}:** {description}")
    
    md_lines.extend([
        "",
        "## è¯æ³•å•å…ƒè¡¨",
        "",
        "| åºå· | ç±»å‹ | å€¼ | ä½ç½® |",
        "|------|------|-----|------|"
    ])
    
    # æ·»åŠ è¯æ³•å•å…ƒè¯¦ç»†ä¿¡æ¯
    for i, (token_type, value, position) in enumerate(detailed_tokens):
        md_lines.append(f"| {i+1} | {token_type} | {value} | {position} |")
    
    md_lines.extend([
        "",
        "## è¯æ³•åˆ†æç»“æœç»Ÿè®¡",
        "",
        f"- **æ€»è¯æ³•å•å…ƒæ•°:** {len(detailed_tokens)}",
        f"- **å…³é”®å­—æ•°é‡:** {sum(1 for t in detailed_tokens if t[0] in ['int', 'float', 'char', 'void', 'if', 'else', 'while', 'for', 'switch', 'case', 'default'])}",
        f"- **æ ‡è¯†ç¬¦æ•°é‡:** {sum(1 for t in detailed_tokens if t[0] == 'id')}",
        f"- **æ•°å­—å¸¸é‡æ•°é‡:** {sum(1 for t in detailed_tokens if t[0] == 'num')}",
        f"- **è¿ç®—ç¬¦æ•°é‡:** {sum(1 for t in detailed_tokens if t[0] not in ['id', 'num'] and t[0] not in ['int', 'float', 'char', 'void', 'if', 'else', 'while', 'for', 'switch', 'case', 'default'])}"
    ])
    
    return "\n".join(md_lines)

def analyze_lr1_with_preprocessing(grammar_key, input_string, use_preprocessing=True):
    """ä¸»è¦çš„åˆ†æå‡½æ•°ï¼Œæ”¯æŒCè¯­è¨€ä»£ç é¢„å¤„ç†"""
    if not grammar_key or not input_string.strip():
        return "è¯·é€‰æ‹©æ–‡æ³•å¹¶è¾“å…¥å¾…åˆ†æçš„Cä»£ç ", "", "", None, None, ""
    
    grammar_info = GRAMMARS[grammar_key]
    grammar_number = grammar_info["number"]
    
    # ä¼˜å…ˆä½¿ç”¨Pythonåç«¯å¤„ç†Cè¯­è¨€æ–‡æ³•
    if grammar_key.startswith("Cè¯­è¨€") and PYTHON_BACKEND_AVAILABLE:
        return analyze_with_python_backend(grammar_key, input_string, use_preprocessing)
    
    # å¦åˆ™ä½¿ç”¨åŸæœ‰çš„C++åç«¯æˆ–æ¨¡æ‹Ÿåˆ†æ
    processed_input = input_string.strip()
    lexical_analysis_md = ""
    
    # å¦‚æœé€‰æ‹©çš„æ˜¯Cè¯­è¨€æ–‡æ³•ä¸”å¯ç”¨é¢„å¤„ç†ï¼Œåˆ™è¿›è¡Œè¯æ³•åˆ†æ
    if use_preprocessing and grammar_key.startswith("Cè¯­è¨€"):
        tokens, detailed_tokens, lexical_analysis_result = preprocess_c_code(input_string.strip())
        processed_input = tokens
        
        # ç”Ÿæˆè¯æ³•åˆ†æMarkdown
        lexical_analysis_md = generate_lexical_analysis_markdown(input_string.strip(), detailed_tokens, lexical_analysis_result)
        
        # å¦‚æœé¢„å¤„ç†åçš„ç»“æœä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹è¾“å…¥
        if not processed_input:
            processed_input = input_string.strip()
    
    # è¿è¡Œåˆ†æ
    console_output, table_content, analysis_content, dfa_image_path, syntax_tree_path = compile_and_run_analysis(
        grammar_key, processed_input
    )
    
    # åœ¨æ§åˆ¶å°è¾“å‡ºä¸­æ·»åŠ é¢„å¤„ç†ä¿¡æ¯
    if use_preprocessing and grammar_key.startswith("Cè¯­è¨€") and processed_input != input_string.strip():
        original_code = input_string.strip()
        if len(original_code) > 50:  # å¦‚æœä»£ç è¾ƒé•¿ï¼Œæˆªæ–­æ˜¾ç¤º
            original_code = original_code[:50] + "..."
            
        preprocessing_info = f"### ğŸ“ è¯æ³•åˆ†æç»“æœ\nåŸå§‹ä»£ç : `{original_code}`\nè½¬æ¢ä¸º: `{processed_input}`\n\n"
        console_output = preprocessing_info + "```\n" + console_output + "\n```"
    else:
        console_output = "```\n" + console_output + "\n```"
    
    # æ£€æŸ¥æ˜¯å¦åˆ†ææˆåŠŸ
    if "åˆ†æå¤±è´¥" in console_output or "é”™è¯¯" in console_output or "åŸå§‹æ–‡æ³•" in console_output:
        # å¦‚æœåˆ†æå¤±è´¥ï¼Œå¯èƒ½æ˜¯å› ä¸ºC++ç¨‹åºä¸æ”¯æŒè¯¥æ–‡æ³•ç¼–å·
        if grammar_key.startswith("Cè¯­è¨€"):
            return console_output, "", "", "", None, lexical_analysis_md
    
    return console_output, table_content, analysis_content, dfa_image_path, syntax_tree_path, lexical_analysis_md

def analyze_lr1(grammar_key, input_string):
    """ä¸»è¦çš„åˆ†æå‡½æ•°"""
    if not grammar_key or not input_string.strip():
        return "è¯·é€‰æ‹©æ–‡æ³•å¹¶è¾“å…¥å¾…åˆ†æçš„å­—ç¬¦ä¸²", "", "", None, None
    
    # è¿è¡Œåˆ†æ
    console_output, table_content, analysis_content, dfa_image_path, syntax_tree_path = compile_and_run_analysis(
        grammar_key, input_string.strip()
    )
    
    return console_output, table_content, analysis_content, dfa_image_path, syntax_tree_path

def get_example_input(grammar_key):
    """æ ¹æ®é€‰æ‹©çš„æ–‡æ³•è¿”å›ç¤ºä¾‹è¾“å…¥"""
    if grammar_key in GRAMMARS:
        return GRAMMARS[grammar_key]["example_input"]
    return ""

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    # è‡ªå®šä¹‰CSSæ ·å¼
    css = """
    .grammar-info {
        background-color: #f8f9fa;
        border: 1px solid #dee2e6;
        border-radius: 0.25rem;
        padding: 0.75rem;
        margin-bottom: 0.75rem;
        font-size: 0.9em;
    }
    
    .output-section {
        margin-top: 0.5rem;
    }
    
    .tab-content {
        min-height: 400px;
    }
    
    .code-editor {
        font-family: 'Courier New', monospace;
        min-height: 150px;
        font-size: 16px !important;
    }
    
    .compact-header {
        margin-bottom: 0.5rem;
    }
    
    .compact-container {
        gap: 0.5rem;
    }
    
    .example-btn {
        background-color: #ffc107 !important;
        font-weight: bold !important;
        color: #343a40 !important;
        margin-bottom: 0.5rem !important;
    }
    
    .analyze-btn {
        background-color: #007bff !important;
        font-weight: bold !important;
        margin-top: 0.5rem !important;
        margin-bottom: 1rem !important;
    }
    
    .console-output {
        border: none !important;
        background: transparent !important;
        padding: 0px !important;
    }
    
    .console-output > div {
        border: none !important;
        background: transparent !important;
    }
    
    .left-panel {
        min-width: 400px !important;
        max-width: 550px !important;
    }
    
    .right-panel {
        min-width: 500px !important;
    }
    """
    
    with gr.Blocks(css=css, title="Cè¯­è¨€LR(1)è¯­æ³•åˆ†æå™¨") as demo:
        gr.Markdown("# Cè¯­è¨€LR(1)è¯­æ³•åˆ†æå™¨", elem_classes=["compact-header"])
        
        with gr.Row(equal_height=False):
            # å·¦ä¾§è¾“å…¥é¢æ¿
            with gr.Column(scale=1, elem_classes=["left-panel"]):
                grammar_dropdown = gr.Dropdown(
                    choices=list(GRAMMARS.keys()),
                    value=list(GRAMMARS.keys())[0],
                    label="é€‰æ‹©æ–‡æ³•",
                )
                
                grammar_info = gr.Markdown(
                    value=f"**äº§ç”Ÿå¼:**\n" + "\n".join([f"- {p}" for p in GRAMMARS[list(GRAMMARS.keys())[0]]["productions"]]),
                    elem_classes=["grammar-info"]
                )
                
                example_btn = gr.Button("ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹", variant="secondary", elem_classes=["example-btn"])
                
                input_string = gr.Code(
                    value=GRAMMARS[list(GRAMMARS.keys())[0]]["example_input"],
                    language="c",
                    label="ä»£ç è¾“å…¥",
                    elem_classes=["code-editor"]
                )
                
                preprocessing_checkbox = gr.Checkbox(
                    value=True,
                    label="è¯æ³•é¢„å¤„ç†",
                    info="è‡ªåŠ¨å°†æ ‡è¯†ç¬¦è½¬æ¢ä¸º'id'ï¼Œæ•°å­—è½¬æ¢ä¸º'num'"
                )
                
                # å°†åˆ†ææŒ‰é’®ç½®äºåº•éƒ¨
                analyze_btn = gr.Button("ğŸš€ å¼€å§‹åˆ†æ", variant="primary", elem_classes=["analyze-btn"])
            
            # å³ä¾§ç»“æœé¢æ¿
            with gr.Column(scale=1, elem_classes=["right-panel"]):
                with gr.Tabs() as tabs:

                    with gr.TabItem("è¯æ³•åˆ†æ"):
                        lexical_analysis_output = gr.Markdown(
                            value="ç‚¹å‡»'å¼€å§‹åˆ†æ'æŸ¥çœ‹è¯æ³•åˆ†æç»“æœ"
                        )
                    with gr.TabItem("LR(1)åˆ†æè¡¨"):
                        table_output = gr.Markdown(
                            value="ç‚¹å‡»'å¼€å§‹åˆ†æ'ç”Ÿæˆåˆ†æè¡¨"
                        )
                    
                    with gr.TabItem("åˆ†æè¿‡ç¨‹"):
                        analysis_output = gr.Markdown(
                            value="ç‚¹å‡»'å¼€å§‹åˆ†æ'æŸ¥çœ‹åˆ†æè¿‡ç¨‹"
                        )
                    
                    with gr.TabItem("DFAçŠ¶æ€å›¾"):
                        dfa_image = gr.Image(
                            type="filepath",
                            label="DFAçŠ¶æ€è½¬æ¢å›¾"
                        )
                    
                    with gr.TabItem("è¯­æ³•åˆ†ææ ‘"):
                        syntax_tree_image = gr.Image(
                            type="filepath",
                            label="è¯­æ³•æ ‘å¯è§†åŒ–"
                        )
                    with gr.TabItem("æ§åˆ¶å°è¾“å‡º"):
                        console_output = gr.Markdown(
                            value="ç‚¹å‡»'å¼€å§‹åˆ†æ'æŸ¥çœ‹æ‰§è¡Œç»“æœ",
                            elem_classes=["console-output"]
                        )

        
        # äº‹ä»¶å¤„ç†
        def update_grammar_info(grammar_key):
            if grammar_key in GRAMMARS:
                productions = GRAMMARS[grammar_key]["productions"]
                info_text = f"**äº§ç”Ÿå¼:**\n" + "\n".join([f"- {p}" for p in productions])
                return info_text
            return "è¯·é€‰æ‹©æ–‡æ³•"
        
        # ç»‘å®šäº‹ä»¶
        grammar_dropdown.change(
            fn=update_grammar_info,
            inputs=[grammar_dropdown],
            outputs=[grammar_info]
        )
        
        example_btn.click(
            fn=get_example_input,
            inputs=[grammar_dropdown],
            outputs=[input_string]
        )
        
        analyze_btn.click(
            fn=analyze_lr1_with_preprocessing,
            inputs=[grammar_dropdown, input_string, preprocessing_checkbox],
            outputs=[console_output, table_output, analysis_output, dfa_image, syntax_tree_image, lexical_analysis_output]
        )
        
        # ç®€åŒ–çš„ä½¿ç”¨è¯´æ˜
        with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("""
            **åŸºæœ¬æ“ä½œ**ï¼šé€‰æ‹©æ–‡æ³• â†’ è¾“å…¥ä»£ç  â†’ å¼€å§‹åˆ†æ
            
            **æ”¯æŒçš„æ–‡æ³•**ï¼š
            - åŸºç¡€æ–‡æ³•(1-6): ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•åŸºç¡€ç¤ºä¾‹
            - Cè¯­è¨€æ–‡æ³•(7-11): èµ‹å€¼è¯­å¥ã€ifè¯­å¥ã€å˜é‡å£°æ˜ã€whileå¾ªç¯ã€ç®—æœ¯è¡¨è¾¾å¼
            """)
    
    return demo

def analyze_with_python_backend(grammar_key, input_string, use_preprocessing=True):
    """ä½¿ç”¨Pythonåç«¯è¿›è¡Œåˆ†æ"""
    if not PYTHON_BACKEND_AVAILABLE:
        return "Pythonåç«¯ä¸å¯ç”¨", "", "", None, None, ""
    
    grammar_info = GRAMMARS[grammar_key]
    grammar_number = grammar_info["number"]
    
    # ç›´æ¥ä½¿ç”¨è¾“å…¥å­—ç¬¦ä¸²ï¼Œä¸éœ€è¦é¢å¤–çš„é¢„å¤„ç†åˆ†å‰²
    processed_input = input_string.strip()
    lexical_analysis_md = ""
    
    # è¯æ³•é¢„å¤„ç† - è¿”å›è¯æ³•å•å…ƒåˆ—è¡¨
    if use_preprocessing and grammar_key.startswith("Cè¯­è¨€"):
        tokens, detailed_tokens, lexical_analysis_result = preprocess_c_code(input_string.strip())
        processed_input = tokens
        
        # ç”Ÿæˆè¯æ³•åˆ†æMarkdown
        lexical_analysis_md = generate_lexical_analysis_markdown(input_string.strip(), detailed_tokens, lexical_analysis_result)
        
        if not processed_input:
            processed_input = input_string.strip()
        print(f"é¢„å¤„ç†åçš„è¯æ³•å•å…ƒ: {processed_input}")
    
    try:
        # åˆ›å»ºæ–‡æ³•å’Œåˆ†æå™¨
        grammar = create_c_grammar(grammar_number)
        parser = LR1Parser(grammar)
        
        # è¿›è¡Œè¯­æ³•åˆ†æ - ç›´æ¥ä¼ å…¥tokenåˆ—è¡¨ï¼Œä¸éœ€è¦å†æ¬¡å¤„ç†
        # processed_inputå·²ç»æ˜¯è¯æ³•å•å…ƒåˆ—è¡¨æˆ–å­—ç¬¦ä¸²
        success, message, steps = parser.parse(processed_input)
        
        # ç”Ÿæˆåˆ†æè¡¨
        table_content = generate_lr1_table_markdown(parser, grammar)
        
        # ç”Ÿæˆåˆ†æè¿‡ç¨‹
        analysis_content = generate_analysis_steps_markdown(steps, success, message)
        
        # ç”ŸæˆDFAå›¾
        dfa_visualizer = DFAVisualizer(parser)
        dfa_path = OUTCOME_DIR / f"python_dfa_grammar{grammar_number}.png"
        dfa_result = dfa_visualizer.generate_dfa_graph(dfa_path)
        
        # ç”Ÿæˆè¯­æ³•æ ‘
        syntax_tree_path = None
        if success and steps:
            tree_builder = SyntaxTreeBuilder(grammar)
            tree_root = tree_builder.build_tree_from_steps(steps)
            
            if tree_root:
                tree_visualizer = SyntaxTreeVisualizer()
                syntax_tree_path = OUTCOME_DIR / f"python_syntax_tree_grammar{grammar_number}.png"
                tree_result = tree_visualizer.generate_tree_graph(tree_root, syntax_tree_path)
                if tree_result:
                    syntax_tree_path = str(syntax_tree_path)
        
        # ç”Ÿæˆæ§åˆ¶å°è¾“å‡º
        console_output = f"""
Python LR(1)åˆ†æå™¨ç»“æœ

ğŸ“ è¯æ³•åˆ†æ: {input_string.strip()} â†’ {processed_input}

æ–‡æ³•ç¼–å·: {grammar_number}
æ–‡æ³•äº§ç”Ÿå¼:
{chr(10).join([f"{i}. {prod}" for i, prod in enumerate(grammar.productions)])}

TokenåŒ–ç»“æœ: {processed_input if isinstance(processed_input, list) else parser.tokenize(processed_input)}

åˆ†æç»“æœ: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}
æ¶ˆæ¯: {message}

æ€»çŠ¶æ€æ•°: {len(parser.states)}
åˆ†ææ­¥éª¤æ•°: {len(steps)}

âœ… LR(1)åˆ†æè¡¨å·²ç”Ÿæˆ
âœ… åˆ†æè¿‡ç¨‹å·²è®°å½•
âœ… DFAçŠ¶æ€å›¾å·²ç”Ÿæˆ
{'âœ… è¯­æ³•æ ‘å·²ç”Ÿæˆ' if syntax_tree_path else 'âš ï¸ è¯­æ³•æ ‘ç”Ÿæˆå¤±è´¥'}

"""
        
        return console_output, table_content, analysis_content, dfa_result, syntax_tree_path, lexical_analysis_md
        
    except Exception as e:
        import traceback
        error_msg = f"""Pythonåç«¯åˆ†æå¤±è´¥: {str(e)}

è¯¦ç»†é”™è¯¯ä¿¡æ¯:
{traceback.format_exc()}"""
        return error_msg, "", "", None, None, lexical_analysis_md
